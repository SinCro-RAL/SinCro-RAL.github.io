<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SinCro.">
  <meta name="keywords" content="SinCro, CroCo, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Single-View 3D-Aware Representations for Reinforcement Learning by Cross-View Neural Radiance Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JXG8B5JDMQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-JXG8B5JDMQ');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Single-View 3D-Aware Representations for Reinforcement Learning by Cross-View Neural Radiance Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dscho1234.github.io">Daesol Cho</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://syeon-yoo.github.io/">Seungyeon Yoo</a><sup>2,*</sup>,</span>
            <span class="author-block">
              Dongseok Shim<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://larr.snu.ac.kr/">H. Jin Kim</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Georgia Institute of Technology, </span>
            <span class="author-block"><sup>2</sup> Seoul National University, </span>
            <span class="author-block"><sup>3</sup> Sony Group Corporation</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=Q1QAKt0e9DY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="teaser-crop">
        <video id="teaser" autoplay muted loop playsinline>
          <source src="./static/videos/sincro_teaser.mp4" type="video/mp4">
        </video>
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">SinCro</span> enables superior RL performance from single-view RGB inputs,
        robust even under viewpoint perturbations, without requiring camera poses or synchronized multi-view images.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning (RL) has enabled robots to develop complex skills, 
            but its success in image-based tasks often depends on effective representation learning. 
            Prior works have primarily focused on 2D representations, often overlooking the inherent 
            3D geometric structure of the world, or have attempted to learn 3D representations that 
            require extensive resources such as synchronized multi-view images even during deployment. 
            To address these issues, we propose a novel RL framework that extracts 3D-aware representations 
            from single-view RGB input, without requiring camera pose or synchronized multi-view images 
            during the downstream RL. 
            Our method employs an autoencoder architecture, using a masked Vision Transformer (ViT) 
            as the encoder and a latent-conditioned Neural Radiance Fields (NeRF) as the decoder, 
            trained with cross-view completion to implicitly capture fine-grained, 3D geometry-aware 
            representations. Additionally, we utilize a time contrastive loss that further regularizes 
            the learned representation for consistency across different viewpoints, which enables 
            viewpoint-robust robot manipulations. Our method significantly enhances the RL agent's 
            performance both in simulation and real-world experiments, demonstrating superior 
            effectiveness compared to prior 3D-aware representation-based methods, 
            even when using only single-view RGB images during deployment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <!-- <video id="teaser" controls playsinline height="100%">
            <source src="./static/videos/SinCro_RA-L_final_video.mp4"
                    type="video/mp4">
          </video> -->
          <iframe src="https://www.youtube.com/embed/Q1QAKt0e9DY?rel=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Rendering. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Novel view & dynamic scene rendering with single-view input</h2>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">SinCro</span> successfully renders the dynamic scene, even in the unseen viewpoints.
            In contrast, all baselines show jittering or fail to capture fine-grained details of the environment.
            This demonstrates that <span class="dnerf">SinCro</span> captures the essential 3D information of the environment,
            providing 3D-aware representation.
          </p>
        </div>
        <div class="content has-text-centered">
          <div class="rendering-crop">
            <video id="replay-video"
                   autoplay
                   muted
                   preload
                   playsinline
                   loop
                   width="100%">
              <source src="./static/videos/rendering.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Rendering. -->

    <!-- Downstream RL. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Real-world downstream task performance</h2>

        <!-- RL evaluation. -->
        <h3 class="title is-4">RL evaluation</h3>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">SinCro</span> successfully performs downstream tasks only with a single-view image.
            In contrast, baselines usually fail to perform the tasks, despite using multiple viewpoints.
          </p>
        </div>
        <div class="content has-text-centered">
          <div class="rendering-crop">
            <video id="replay-video"
                   autoplay
                   muted
                   preload
                   playsinline
                   loop
                   width="100%">
              <source src="./static/videos/RL_evaluation.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <br/>
        <!--/ RL evaluation. -->

        <!-- RL evaluation under perturbed viewpoints. -->
        <h3 class="title is-4">RL evaluation under perturbed viewpoints</h3>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">SinCro</span> remains robust to viewpoint perturbations, consistently succeeding in downstream tasks,
            whereas baselines mostly fail.
            Since perturbed viewpoints are never observed during the pre-training and the RL training,
            this robustness showcases the advantages of the proposed 3D-aware representation.
          </p>
        </div>
        <div class="content has-text-centered">
          <div class="rendering-crop">
            <video id="replay-video"
                   autoplay
                   muted
                   preload
                   playsinline
                   loop
                   width="100%">
              <source src="./static/videos/RL_evaluation_perturbed.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ RL evaluation under perturbed viewpoints. -->

      </div>
    </div>
    <!--/ Downstream RL. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre> -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            <!-- This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website. -->
            This page was built with <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
